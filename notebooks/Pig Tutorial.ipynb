{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pig Best Practices and Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "- Discuss Assingments\n",
    "- Timing your UDF's\n",
    "- Combiner when and when not to use (Pre requisite : Algebraic interface)\n",
    "- Drop nulls before join (Demo only if a large dataset on cluster is available)\n",
    "-  Combine small input files (Demo only if a large dataset on cluster is available) \n",
    "- Replicated Joins (map side join) (demo  only if a large dataset on cluster  is available)\n",
    "- Prefer DISTINCT over GROUP BY/GENERATE\n",
    "- Compress the Results of Intermediate Jobs\n",
    "- Generic Big data Parquet files ( demo only if a large dataset on cluster  is available)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assingment 1\n",
    "Take any data set and perform a data shuffle and split the data into two parts (90/10 split for training/testing).\n",
    "\n",
    "### Solution\n",
    "\n",
    "a = load 'dataset' using PigStorage(',');\n",
    "a = foreach a generate RANDOM(), *;\n",
    "\n",
    "split a into training if $0 <= 0.9, test if $0 > 0.9;\n",
    "\n",
    "training = group training all;\n",
    "training = foreach training generate flatten($1);\n",
    "training = foreach training generate $1 ..;\n",
    "\n",
    "test = group test all;\n",
    "test = foreach test generate flatten($1);\n",
    "test = foreach test generate $1 ..;\n",
    "\n",
    "store training into 'training' using PigStorage(',');\n",
    "store test into 'test' using PigStorage(',');\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notes \n",
    "Project-Range Expressions\n",
    "Project-range ( .. ) expressions can be used to project a range of columns from input. For example:\n",
    "\n",
    ".. $x : projects columns $0 through $x, inclusive\n",
    "\n",
    "$x .. : projects columns through end, inclusive\n",
    "\n",
    "$x .. $y : projects columns through $y, inclusive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "Download data from http://stat-computing.org/dataexpo/2009/the-data.html for year 2007\n",
    "\n",
    "a. Write a Pig script wich imports the data and filters out all the delayed flights.\n",
    "b. Write a UDF to find out the hour of departure from the field CRSDepTime\n",
    "c. Write a UDF to find out the days from the nearest holiday for the date(Year, Month, DayOfMonth) fields.\n",
    "\n",
    "Note : treat the following dates as holidays\n",
    "    \n",
    "holidays = [\n",
    "        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n",
    "        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25)]\n",
    "\n",
    "\n",
    "Register 'util.py' USING jython as util;\n",
    "-- load airline data from specified year\n",
    "airline = load 'delay_2007.csv' using PigStorage(',') \n",
    "    as (Year: int, Month: int, DayOfMonth: int, DayOfWeek: int, DepTime: chararray, \n",
    "        CRSDepTime: chararray, ArrTime, CRSArrTime, Carrier: chararray, FlightNum, TailNum, ActualElapsedTime, \n",
    "        CRSElapsedTime, AirTime, ArrDelay, DepDelay: int, Origin: chararray, Dest: chararray, Distance: int, \n",
    "        TaxiIn, TaxiOut, Cancelled: int, CancellationCode, Diverted, CarrierDelay, WeatherDelay, \n",
    "        NASDelay, SecurityDelay, LateAircraftDelay);\n",
    "\n",
    "-- keep only instances where flight was not cancelled and originate at ORD\n",
    "airline_flt = filter airline by Cancelled == 0 and Origin == '$airport_code';\n",
    "\n",
    "-- Keep only the needed fields\n",
    "$data = foreach airline_flt generate DepDelay as delay, Month, DayOfMonth, DayOfWeek, \n",
    "                                     util.get_hour(CRSDepTime) as hour, Distance, Carrier, Dest,\n",
    "                                     util.days_from_nearest_holiday(Year, Month, DayOfMonth) as hdays;\n",
    "\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "                                          \n",
    "#### get hour-of-day from HHMM field\n",
    "#### more on zfill : https://www.tutorialspoint.com/python/string_zfill.htm\n",
    "@outputSchema(\"value: int\")\n",
    "def get_hour(val):\n",
    "  return int(val.zfill(4)[:2])\n",
    "\n",
    "                                          \n",
    "##### this array defines the dates of holiday in 2007\n",
    "holidays = [\n",
    "        date(2007, 1, 1), date(2007, 1, 15), date(2007, 2, 19), date(2007, 5, 28), date(2007, 6, 7), date(2007, 7, 4), \\\n",
    "        date(2007, 9, 3), date(2007, 10, 8), date(2007, 11, 11), date(2007, 11, 22), date(2007, 12, 25)]\n",
    "\n",
    "#### get number of days from nearest holiday\n",
    "@outputSchema(\"days: int\")\n",
    "def days_from_nearest_holiday(year, month, day):\n",
    "  d = date(year, month, day)\n",
    "  x = [(abs(d-h)).days for h in holidays]\n",
    "  return min(x)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assingment 3 Natural Language Processing, Text Processing\n",
    "\n",
    "\n",
    "Get data from https://www.kaggle.com/zynicide/wine-reviews/data\n",
    "From the wine data find the top 5 words used to describe each wine for each country.\n",
    "Note: You should write Python UDF/UADF for NLP proceessing.\n",
    "\n",
    "Solution 3\n",
    "\n",
    "\n",
    "\n",
    "REGISTER 'a4.py' USING jython AS nltk_udfs;\n",
    "\n",
    "winedata =  LOAD 'winedata' \n",
    "         USING PigStorage(',') AS (id:chararray, country:chararray, description:chararray, designation:chararray, points:chararray, price:chararray, province:chararray, region_1:chararray, region_2:chararray, variety:chararray, winery:chararray);\n",
    "DESCRIBE winedata;\n",
    "-- Group the tweets by place name and use a CPython UDF to find the top 5 bigrams\n",
    "-- for each of these places.\n",
    "bigrams_by_place = FOREACH (GROUP winedata BY country) GENERATE\n",
    "                        group AS country:chararray, \n",
    "                        nltk_udfs.top_5_bigrams(winedata.description), \n",
    "                        COUNT(winedata) AS sample_size;\n",
    "\n",
    "top_100_places = LIMIT (ORDER bigrams_by_place BY sample_size DESC) 100;\n",
    "\n",
    "STORE top_100_places INTO 'a4-output';\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/usr/local/lib/python2.7/')\n",
    "from collections import defaultdict\n",
    "import re \n",
    "@outputSchema('word:chararray')\n",
    "def special_tokenize(input):\n",
    "    #splitting camel-case here\n",
    "    temp_data = re.sub(r'(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])',\" \",input)\n",
    "    tokens = temp_data.encode('utf-8')\n",
    "    final_tokens = tokens.split()\n",
    "    return final_tokens  \n",
    "@outputSchema(\"top_five:bag{t:(bigram:chararray)}\")\n",
    "def top_5_bigrams(wines):\n",
    "    dict = defaultdict(int)\n",
    "    tokenized_wines = [ special_tokenize(t[0]) for t in wines ]\n",
    "    for tw in tokenized_wines:\n",
    "        for w in tw:\n",
    "            dict[w] += 1\n",
    "    sd = sorted(dict.items(), key=lambda x: (x[1],x[0]), reverse=True)\n",
    " \n",
    "    return sd[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Assignment 4\n",
    "# big data - list all google books data links https://gist.github.com/Kroid/d92a242b374043353ea6\n",
    "\n",
    "Finds word frequencies (probability that a random word is the given word) using the Google Books corpus.\n",
    "\n",
    "Download data using : \n",
    "- curl http://storage.googleapis.com/books/ngrams/books/googlebooks-eng-all-1gram-20120701-1.gz -o gb.gz\n",
    "- gunzip gb.gz\n",
    "\n",
    "\n",
    "a. Load the word occurrence data\n",
    "b. Ignore case, combining the occurrences of different capitalizations of the same word ex. \"quantity\", \"Quantity\", and \"QUANTITY\" all get combined    \n",
    "c. Find the word frequencies (probability that a random word is the given word) by normalizing the occurrences column against the total number of occurrence\n",
    "\n",
    "\n",
    "%default INPUT_PATH 'gb'\n",
    "%default OUTPUT_PATH '1gram'\n",
    "\n",
    "-- Timeframe to take word occurrences from\n",
    "-- The Google Books Ngrams V2 dataset has data up to 2008\n",
    "\n",
    "%default START_YEAR '2004'\n",
    "%default END_YEAR '9999'\n",
    "\n",
    "-- Load the word occurrence data\n",
    "\n",
    "words           =   LOAD '$INPUT_PATH' USING PigStorage('\\t') AS (word: chararray, year: int, occurrences: int);\n",
    "\n",
    "-- Filter out years outside of the specified timeframe\n",
    "-- Filter out words that have non-letter characters in them\n",
    "\n",
    "filtered_words  =   FILTER words BY (\n",
    "                        year >= $START_YEAR AND \n",
    "                        year <= $END_YEAR AND \n",
    "                        word matches '^[A-Za-z]+$'\n",
    "                    );\n",
    "\n",
    "-- Ignore case, combining the occurrences of different capitalizations of the same word\n",
    "-- ex. \"quantity\", \"Quantity\", and \"QUANTITY\" all get combined\n",
    "\n",
    "words_lower_with_dups   =   FOREACH filtered_words GENERATE LOWER(word) AS word, year, occurrences;\n",
    "words_lower_grouped     =   GROUP words_lower_with_dups BY (word, year);\n",
    "words_lower             =   FOREACH words_lower_grouped GENERATE \n",
    "                                group.word AS word, \n",
    "                                group.year AS year, \n",
    "                                SUM(words_lower_with_dups.occurrences) AS occurrences;\n",
    "\n",
    "-- Get the number of occurrences for each word over the entire timeframe\n",
    "\n",
    "words_lower_over_time   =   GROUP words_lower BY word;\n",
    "word_totals             =   FOREACH words_lower_over_time GENERATE \n",
    "                                group AS word, \n",
    "                                SUM(words_lower.occurrences) AS occurrences;\n",
    "\n",
    "-- Find the word frequencies (probability that a random word is the given word)\n",
    "-- by normalizing the occurrences column against the total number of occurrences\n",
    "\n",
    "all_word_totals         =   GROUP word_totals ALL;\n",
    "stats                   =   FOREACH all_word_totals GENERATE \n",
    "                                COUNT(word_totals) AS num_unique_words,\n",
    "                                SUM(word_totals.occurrences) AS total_num_words;\n",
    "\n",
    "word_frequencies        =   FOREACH word_totals GENERATE \n",
    "                                word AS word, \n",
    "                                occurrences AS occurrences, \n",
    "                                (double)occurrences / (double)stats.total_num_words AS frequency: double;\n",
    "word_frequencies_sorted =   ORDER word_frequencies BY frequency DESC;\n",
    "\n",
    "rmf $OUTPUT_PATH/dictionary;\n",
    "STORE word_frequencies_sorted INTO '$OUTPUT_PATH/dictionary' USING PigStorage('\\t');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ILLUSTRATE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A = LOAD 'dataset' USING PigStorage(',') \n",
    "            AS (mjob: chararray, fjob: chararray, reason: chararray, guardian: chararray, traveltime: int);\n",
    "DUMP A;\n",
    "DESCRIBE A;\n",
    "\n",
    "B = GROUP A BY (traveltime, guardian);\n",
    "C = FOREACH B GENERATE group, COUNT(A) as count;\n",
    "O = ORDER C BY traveltime  DESC;\n",
    "\n",
    "ILLUSTRATE O;\n",
    "DUMP O;\n",
    "DESCRIBE O;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing UDF's\n",
    "\n",
    "- Find properties in cat pig-0.14.0/conf/pig.properties \n",
    "\n",
    "- set pig.udf.profile property true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combiners\n",
    "\n",
    "The Pig combiner is an optimizer that is invoked when the statements in your scripts are arranged in certain ways. The examples below demonstrate when the combiner is used and not used. Whenever possible, make sure the combiner is used as it frequently yields an order of magnitude improvement in performance.\n",
    "\n",
    "\n",
    "#### Small discussion on Algebraic interface\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A = load 'studenttab10k' as (name, age, gpa);\n",
    "B = group A by age;\n",
    "C = foreach B generate ABS(SUM(A.gpa)), COUNT(org.apache.pig.builtin.Distinct(A.name)), (MIN(A.gpa) + MAX(A.gpa))/2, group.age;\n",
    "explain C;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above example:\n",
    "\n",
    "The GROUP statement can be referred to as a whole or by accessing individual fields (as in the example).\n",
    "The GROUP statement and its elements can appear anywhere in the projection.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In the above example, a variety of expressions can be applied to algebraic functions including:\n",
    "\n",
    "A column transformation function such as ABS can be applied to an algebraic function SUM.\n",
    "An algebraic function (COUNT) can be applied to another algebraic function (Distinct), but only the inner function is computed using the combiner.\n",
    "A mathematical expression can be applied to one or more algebraic functions.\n",
    "You can check if the combiner is used for your query by running EXPLAIN on the FOREACH alias as shown above. \n",
    "You should see the combine section in the MapReduce part of the plan:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The combiner is generally not used if there is any operator that comes between the GROUP and FOREACH statements \n",
    "in the execution plan. Even if the statements are next to each other in your script,\n",
    "the optimizer might rearrange them. \n",
    "\n",
    "In this example, the optimizer will push FILTER above FOREACH which will prevent the use of the combiner:\n",
    "\n",
    "A = load 'studenttab10k' as (name, age, gpa);\n",
    "B = group A by age;\n",
    "C = foreach B generate group, COUNT (A);\n",
    "D = filter C by group.age <30;\n",
    "\n",
    "Note that the script above can be made more efficient by performing filtering before the GROUP statement:\n",
    "\n",
    "A = load 'studenttab10k' as (name, age, gpa);\n",
    "B = filter A by age <30;\n",
    "C = group B by age;\n",
    "D = foreach C generate group, COUNT (B);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop nulls before join\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A = load 'myfile' as (t, u, v);\n",
    "B = load 'myotherfile' as (x, y, z);\n",
    "C = join A by t, B by x;\n",
    "\n",
    "\n",
    "\n",
    "A = load 'myfile' as (t, u, v);\n",
    "B = load 'myotherfile' as (x, y, z);\n",
    "A1 = filter A by t is not null;\n",
    "B1 = filter B by x is not null;\n",
    "C = join A1 by t, B1 by x;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicated Joins\n",
    "\n",
    "Make sure that the table with the largest number of tuples per key is the last table in your query. Can give upto 10x improvement."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "small = load 'small_file' as (t, u, v);\n",
    "large = load 'large_file' as (x, y, z);\n",
    "C = join small by t, large by x;\n",
    "\n",
    "\n",
    "big = LOAD 'big_data' AS (b1,b2,b3);\n",
    "\n",
    "tiny = LOAD 'tiny_data' AS (t1,t2,t3);\n",
    "\n",
    "mini = LOAD 'mini_data' AS (m1,m2,m3);\n",
    "\n",
    "C = JOIN big BY b1, tiny BY t1, mini BY m1 USING 'replicated';\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefer DISTINCT over GROUP BY/GENERATE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A = load 'myfile' as (t, u, v);\n",
    "B = foreach A generate u;\n",
    "C = group B by u;\n",
    "D = foreach C generate group as uniquekey;\n",
    "dump D; \n",
    "\n",
    "\n",
    "\n",
    "Example using DISTINCT:\n",
    "\n",
    "A = load 'myfile' as (t, u, v);\n",
    "B = foreach A generate u;\n",
    "C = distinct B;\n",
    "dump C; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Columnar storage layout: A query can examine and perform calculations on all values for a column while reading only a small fraction of the data from a data file or table.\n",
    "- Flexible compression options: The data can be compressed with any of several codecs. Different data files can be compressed differently. The compression is transparent to applications that read the data files.\n",
    "- Innovative encoding schemes: Sequences of identical, similar, or related data values can be represented in ways that save disk space and memory, yet require little effort to decode. The encoding schemes provide an extra level of space savings beyond the overall compression for each data file.\n",
    "- Large file size: The layout of Parquet data files is optimized for queries that process large volumes of data, with individual files in the multi-megabyte or even gigabyte range."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A = LOAD '/test-table' USING parquet.pig.ParquetLoader AS (x: int, y int);\n",
    "\n",
    "store A into '/test-table' USING parquet.pig.ParquetStorer;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress the Results of Intermediate Jobs\n",
    "\n",
    "If your Pig script generates a sequence of MapReduce jobs, you can compress the output of the intermediate jobs using LZO compression. (Use the EXPLAIN operator to determine if your script produces multiple MapReduce Jobs.)\n",
    "\n",
    "\n",
    "By doing this, you will save HDFS space used to store the intermediate data used by PIG and potentially improve query execution speed. In general, the more intermediate data that is generated, the more benefits in storage and speed that result.\n",
    "\n",
    "You can set the value for these properties:\n",
    "\n",
    "- pig.tmpfilecompression - Determines if the temporary files should be compressed or not (set to false by default).\n",
    "- pig.tmpfilecompression.codec - Specifies which compression codec to use. Currently, Pig accepts \"gz\" and \"lzo\" as possible values. However, because LZO is under GPL license (and disabled by default) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-- launch Pig script using lzo compression \n",
    "\n",
    "java -cp $PIG_HOME/pig.jar \n",
    "-Djava.library.path=<path to the lzo library> \n",
    "-Dpig.tmpfilecompression=true \n",
    "-Dpig.tmpfilecompression.codec=lzo org.apache.pig.Main  myscript.pig "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine small input files\n",
    "\n",
    "Processing input (either user input or intermediate input) from multiple small files can be inefficient because a separate map has to be created for each file. Pig can now combined small files so that they are processed as a single map.\n",
    "\n",
    "You can set the values for these properties:\n",
    "\n",
    "- pig.maxCombinedSplitSize – Specifies the size, in bytes, of data to be processed by a single map. Smaller files are combined untill this size is reached.\n",
    "\n",
    "\n",
    "- pig.splitCombination – Turns combine split files on or off (set to “true” by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
